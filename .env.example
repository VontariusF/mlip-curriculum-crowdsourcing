# MLIP Curriculum Crowdsourcing System - Environment Configuration
# Copy this file to .env and fill in your actual values
# 
# SETUP INSTRUCTIONS:
# 1. Complete all prerequisites (see README Prerequisites section)
# 2. Set up Firecrawl locally (see Firecrawl Local Setup)
# 3. Get Together AI API keys (see Together AI API Setup)
# 4. Copy this file to .env and fill in your actual values
# 5. Run: python smoke_test.py to verify setup

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# PostgreSQL database connection settings
# 
# SETUP NOTES:
# - We use port 5433 to avoid conflicts with default PostgreSQL (5432)
# - If port 5433 is in use, change POSTGRES_PORT and update docker-compose.yml
# - Database will be created automatically when you run: docker-compose up -d db
# - Default credentials are safe for local development only
#
# PRODUCTION: Use strong passwords and consider external database hosting
DATABASE_URL=postgresql://username:password@localhost:5433/mlip_curriculum
POSTGRES_HOST=localhost
POSTGRES_PORT=5433
POSTGRES_DB=mlip_curriculum
POSTGRES_USER=username
POSTGRES_PASSWORD=password

# =============================================================================
# FIRECRAWL CONFIGURATION
# =============================================================================
# Firecrawl API settings for web scraping
#
# SETUP NOTES:
# - For local setup: Usually no API key required
# - Local URL: http://localhost:3001 (default)
# - Verify Firecrawl is running: curl http://localhost:3001/health
# - If using cloud version: Get API key from https://www.firecrawl.dev/
#
# TROUBLESHOOTING:
# - Check containers: docker ps
# - Check logs: docker logs firecrawl
# - Restart if needed: docker-compose restart
FIRECRAWL_API_URL=http://localhost:3001
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# =============================================================================
# AGENT 1: SCRAPING AGENT (Llama-3.3-70B-Instruct-Turbo-Free)
# =============================================================================
# Together AI credentials for the scraping agent
#
# SETUP NOTES:
# - Sign up at: https://api.together.xyz/
# - Navigate to Settings â†’ API Keys
# - Create new API key
# - Copy User Key and API Key
# - Free tier available for testing
#
# MODEL INFO:
# - Model: meta-llama/Llama-3.3-70B-Instruct-Turbo-Free
# - Purpose: Intelligent web scraping and content discovery
# - Capabilities: Relevance filtering, content extraction, link discovery
SCRAPING_AGENT_USER_KEY=your_together_ai_user_key_here
SCRAPING_AGENT_API_KEY=your_together_ai_api_key_here
SCRAPING_AGENT_MODEL=meta-llama/Llama-3.3-70B-Instruct-Turbo-Free

# =============================================================================
# AGENT 2: CLASSIFICATION AGENT (Apriel-1.5-15b-Thinker)
# =============================================================================
# Together AI credentials for the classification agent
#
# SETUP NOTES:
# - Uses same Together AI account as Agent 1
# - Model may require enterprise access via ServiceNow AI
# - If ServiceNow AI unavailable, use alternative models (see README)
#
# MODEL INFO:
# - Model: ServiceNow-AI/Apriel-1.5-15b-Thinker
# - Purpose: Document classification and semantic analysis
# - Capabilities: Content analysis, categorization, topic extraction
#
# ALTERNATIVES (if ServiceNow AI unavailable):
# - meta-llama/Llama-3.1-70B-Instruct-Turbo
# - meta-llama/Llama-3.1-8B-Instruct-Turbo
CLASSIFICATION_AGENT_USER_KEY=your_apriel_user_key_here
CLASSIFICATION_AGENT_API_KEY=your_apriel_api_key_here
CLASSIFICATION_AGENT_MODEL=ServiceNow-AI/Apriel-1.5-15b-Thinker

# =============================================================================
# EMBEDDING MODEL CONFIGURATION
# =============================================================================
# Sentence transformer model for generating embeddings
#
# SETUP NOTES:
# - Model downloads automatically on first use
# - Requires internet connection for initial download
# - Model size: ~90MB for all-MiniLM-L6-v2
# - Used for semantic similarity and deduplication
#
# MODEL OPTIONS:
# - sentence-transformers/all-MiniLM-L6-v2 (recommended, fast)
# - sentence-transformers/all-mpnet-base-v2 (better quality, slower)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# BATCH PROCESSING CONFIGURATION
# =============================================================================
# Number of resources to process in each batch
#
# SETUP NOTES:
# - Smaller batches: Lower memory usage, more API calls
# - Larger batches: Better throughput, higher memory usage
# - Start with 10 for testing, increase for production
# - Monitor memory usage and API rate limits
#
# RECOMMENDED VALUES:
# - Testing: 10-20
# - Production: 50-100
BATCH_SIZE=10

# Maximum number of concurrent batches
#
# SETUP NOTES:
# - Higher values: Better throughput, higher resource usage
# - Lower values: More stable, lower resource usage
# - Start with 1-2 for testing
#
# RECOMMENDED VALUES:
# - Testing: 1-2
# - Production: 3-5
MAX_CONCURRENT_BATCHES=2

# Delay between scraping requests (seconds)
#
# SETUP NOTES:
# - Prevents overwhelming target websites
# - Respects robots.txt and rate limiting
# - Higher values: More respectful, slower processing
# - Lower values: Faster processing, may trigger blocks
#
# RECOMMENDED VALUES:
# - Conservative: 3-5 seconds
# - Balanced: 1-3 seconds
# - Aggressive: 0.5-1 seconds (use with caution)
SCRAPING_DELAY_SECONDS=2

# =============================================================================
# TERMINAL HUB CONFIGURATION
# =============================================================================
# Refresh rate for the terminal hub display (seconds)
#
# SETUP NOTES:
# - Higher values: Smoother display, higher CPU usage
# - Lower values: More responsive, lower CPU usage
# - Adjust based on your system performance
#
# RECOMMENDED VALUES:
# - Fast systems: 0.5-1.0 seconds
# - Slower systems: 1.0-2.0 seconds
TERMINAL_HUB_REFRESH_RATE=1.0

# Number of log lines to display in terminal hub
#
# SETUP NOTES:
# - Higher values: More history, higher memory usage
# - Lower values: Less history, lower memory usage
# - Adjust based on your terminal size and preferences
#
# RECOMMENDED VALUES:
# - Small terminals: 20-50 lines
# - Large terminals: 50-100 lines
TERMINAL_HUB_LOG_LINES=50

# =============================================================================
# PERFORMANCE TUNING (OPTIONAL)
# =============================================================================
# Enable embedding caching for better performance
#
# SETUP NOTES:
# - Caching reduces API calls and improves speed
# - Requires additional memory for cache storage
# - Enable for production, disable for testing
# - Cache TTL: Time in seconds before cache expires
#
# RECOMMENDED VALUES:
# - Testing: Disable caching
# - Production: Enable with 10,000+ cache size
# EMBEDDING_CACHE_ENABLED=true
# EMBEDDING_CACHE_SIZE=10000
# EMBEDDING_CACHE_TTL=86400

# Rate limiting for API calls
#
# SETUP NOTES:
# - CLASSIFICATION_DELAY: Delay between classification requests
# - BATCH_DELAY: Delay between full batch processing cycles
# - Adjust based on API rate limits and system performance
#
# RECOMMENDED VALUES:
# - Conservative: 2-5 seconds delays
# - Balanced: 1-2 seconds delays
# - Aggressive: 0.5-1 seconds delays (monitor API limits)
# CLASSIFICATION_DELAY_SECONDS=1
# BATCH_DELAY_SECONDS=30

# =============================================================================
# DEBUGGING AND LOGGING (OPTIONAL)
# =============================================================================
# Set to 'DEBUG' for verbose logging, 'INFO' for normal logging
#
# SETUP NOTES:
# - DEBUG: Shows detailed information for troubleshooting
# - INFO: Shows normal operation information
# - WARNING: Shows only warnings and errors
# - ERROR: Shows only errors
#
# RECOMMENDED VALUES:
# - Development: DEBUG
# - Production: INFO or WARNING
# LOG_LEVEL=INFO

# Enable debug mode for additional logging
#
# SETUP NOTES:
# - Provides additional debugging information
# - May impact performance
# - Use only when troubleshooting issues
#
# RECOMMENDED VALUES:
# - Development: true
# - Production: false
# DEBUG_MODE=false

# =============================================================================
# SECURITY NOTES
# =============================================================================
# IMPORTANT SECURITY CONSIDERATIONS:
#
# 1. NEVER commit this file to version control
#    - The .env file contains sensitive information
#    - Use .gitignore to prevent accidental commits
#    - Use .env.example for sharing configuration structure
#
# 2. Use strong, unique passwords for database
#    - Default passwords are for local development only
#    - Use password managers for production
#    - Rotate passwords regularly
#
# 3. Rotate API keys regularly
#    - Set up key rotation schedule
#    - Monitor API usage for anomalies
#    - Revoke compromised keys immediately
#
# 4. Use environment-specific configurations
#    - Different settings for dev/staging/production
#    - Use secrets management for production
#    - Consider using environment variable injection
#
# 5. Consider using secrets management for production
#    - AWS Secrets Manager, Azure Key Vault, etc.
#    - Avoid hardcoding secrets in configuration files
#    - Use least privilege principle for API access
#
# 6. Monitor and audit access
#    - Log API key usage
#    - Monitor for unusual access patterns
#    - Set up alerts for security events